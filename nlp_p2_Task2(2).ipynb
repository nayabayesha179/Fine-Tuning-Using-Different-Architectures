{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9735291a610246ffac01e447f5586265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ae8b4c2e07f47d883faf0e00ec805d2",
              "IPY_MODEL_d7e6f0c76aa84fccbfb5fb54dbc7321d",
              "IPY_MODEL_7f8853dfef6e4c589480b9701510aa82"
            ],
            "layout": "IPY_MODEL_fc54f5bcdcad4a0185e3ba8db2c51232"
          }
        },
        "9ae8b4c2e07f47d883faf0e00ec805d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0815722f51b43d5abac98eed3ec1b41",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_afd00a79a57c44359abed5f4bfc9ac56",
            "value": "Map:â€‡100%"
          }
        },
        "d7e6f0c76aa84fccbfb5fb54dbc7321d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4837c4f95164540964174407ec40f2a",
            "max": 2231143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c35a1122e9f49118d0227b9d0affb1f",
            "value": 2231143
          }
        },
        "7f8853dfef6e4c589480b9701510aa82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f87c8f96fe0464ba6eaebc446ae054c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a32d14a26b2e4e37b4dae795a8c867e3",
            "value": "â€‡2231143/2231143â€‡[04:36&lt;00:00,â€‡9453.56â€‡examples/s]"
          }
        },
        "fc54f5bcdcad4a0185e3ba8db2c51232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0815722f51b43d5abac98eed3ec1b41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afd00a79a57c44359abed5f4bfc9ac56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4837c4f95164540964174407ec40f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c35a1122e9f49118d0227b9d0affb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f87c8f96fe0464ba6eaebc446ae054c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32d14a26b2e4e37b4dae795a8c867e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d648461710e45fdb5991c6df3c73a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7086f9dcb99434b9b36cbab63b8f100",
              "IPY_MODEL_bf2c3edc6c034d70a33ac1dba31f3eea",
              "IPY_MODEL_28ea46ad92814a55b952778aaf8f587d"
            ],
            "layout": "IPY_MODEL_08a3d686b38d4ee683972eec8b05e764"
          }
        },
        "a7086f9dcb99434b9b36cbab63b8f100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec4968e8a0284182849caf786b77edf6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_88b63cea8e6a43c8be0590de0e1f5abc",
            "value": "Tokenizingâ€‡dataset:â€‡100%"
          }
        },
        "bf2c3edc6c034d70a33ac1dba31f3eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d8cc60f37d5492babb51ae93f759f38",
            "max": 2231143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62bfc2583a7f4c16a2e6a5d27fccc714",
            "value": 2231143
          }
        },
        "28ea46ad92814a55b952778aaf8f587d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22821b1277cd4f81a7751c789b530eb3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d7f22dd90eae433db79bcb8f79d5e449",
            "value": "â€‡2231143/2231143â€‡[56:11&lt;00:00,â€‡577.51â€‡examples/s]"
          }
        },
        "08a3d686b38d4ee683972eec8b05e764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec4968e8a0284182849caf786b77edf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88b63cea8e6a43c8be0590de0e1f5abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d8cc60f37d5492babb51ae93f759f38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62bfc2583a7f4c16a2e6a5d27fccc714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22821b1277cd4f81a7751c789b530eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f22dd90eae433db79bcb8f79d5e449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivWQqFdeNHVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81bf835-b062-46ce-aa0f-a2d3d21f712d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets torch pandas numpy scikit-learn matplotlib seaborn tqdm gradio evaluate rouge-score nltk kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "mm8eq8ZtOxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n"
      ],
      "metadata": {
        "id": "cOxiC0N_OzaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167f3b4a-7fc6-437c-9864-69e5f4f2598f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d nazmussakibrupol/3a2mext\n"
      ],
      "metadata": {
        "id": "6Cw8Ao8OO1XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65097153-c8aa-4efd-bc6d-c1b36f6e4ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/nazmussakibrupol/3a2mext\n",
            "License(s): unknown\n",
            "3a2mext.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 3a2mext.zip -d recipes_dataset"
      ],
      "metadata": {
        "id": "5HrUhuo5O3TB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d429e7c-bf6a-43f6-d7b6-4d5155c5db98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  3a2mext.zip\n",
            "replace recipes_dataset/3A2M_EXTENDED.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: recipes_dataset/3A2M_EXTENDED.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"recipes_dataset/3A2M_EXTENDED.csv\")  # adjust path if different\n",
        "print(df.columns)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "jwZ79f-TO5WE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da27f47a-fd0a-4f73-f906-70f90f839e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['title', 'NER', 'Extended_NER', 'genre', 'label', 'directions'], dtype='object')\n",
            "                                         title  \\\n",
            "0                 \\t Arugula Pomegranate Salad   \n",
            "1               \\t Black Bean And Turkey Chili   \n",
            "2               \\t Finger Lickin' Tofu Nuggets   \n",
            "3  \\t Jerk Beef Stew With Carrots And Tomatoes   \n",
            "4                \\t Pomegranate Couscous Salad   \n",
            "\n",
            "                                                 NER  \\\n",
            "0  [\"baby spinach\", \"baby arugula\", \"pomegranate ...   \n",
            "1  [\"olive oil\", \"yellow onion\", \"garlic\", \"groun...   \n",
            "2  [\"extra firm\", \"almond flour\", \"nutritional ye...   \n",
            "3  [\"olive oil\", \"boneless beef chuck\", \"onion\", ...   \n",
            "4  [\"pomegranate arils\", \"whole wheat couscous\", ...   \n",
            "\n",
            "                                        Extended_NER       genre  label  \\\n",
            "0  ['alfalfa sprouts', 'baby spinach', 'baby arug...  vegetables      4   \n",
            "1  ['one', 'yellow onion', 'tomato paste', 'about...       sides      8   \n",
            "2  ['extra firm', '2', 'coconut oil', 'almond flo...      nonveg      3   \n",
            "3  ['boneless beef chuck', '2', 'Saute', 'onion',...  vegetables      4   \n",
            "4  ['whole wheat couscous', '10 minutes', 'lemon ...  vegetables      4   \n",
            "\n",
            "                                          directions  \n",
            "0  [\"Toss together spinach and arugula, then plac...  \n",
            "1  [\"Dice the onion and mince the garlic. Add the...  \n",
            "2  [\"Wrap the tofu in a clean tea towel and press...  \n",
            "3  [\"Preheat oven to 350 degrees F.\", \"Heat the o...  \n",
            "4  [\"Place couscous in a bowl with 11/2 cups of h...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and process the dataset\n",
        "def prepare_data(csv_path):\n",
        "    \"\"\"Load and prepare recipe data for training\"\"\"\n",
        "    print(\"ðŸ“Š Loading dataset...\")\n",
        "\n",
        "    # Load directly from CSV\n",
        "    dataset = load_dataset(\"csv\", data_files={\"train\": csv_path})\n",
        "\n",
        "    print(\"Dataset sample:\")\n",
        "    print(dataset[\"train\"][0])\n",
        "    print(\"\\nColumns:\", dataset[\"train\"].column_names)\n",
        "\n",
        "    # Process the data\n",
        "    def process_example(example):\n",
        "        title = str(example['title']).strip()\n",
        "        ingredients = example['NER']\n",
        "        instructions = example['directions']\n",
        "\n",
        "        # Process ingredients\n",
        "        if isinstance(ingredients, str) and ingredients.startswith(\"[\") and ingredients.endswith(\"]\"):\n",
        "            try:\n",
        "                ing_list = json.loads(ingredients)\n",
        "                ingr_text = \"\\n\".join(f\"- {i.strip()}\" for i in ing_list if i.strip())\n",
        "            except:\n",
        "                ingr_text = ingredients\n",
        "        else:\n",
        "            ingr_text = str(ingredients)\n",
        "\n",
        "        # Process instructions\n",
        "        if isinstance(instructions, str) and instructions.startswith(\"[\") and instructions.endswith(\"]\"):\n",
        "            try:\n",
        "                inst_list = json.loads(instructions)\n",
        "                inst_text = \"\\n\".join(f\"{i+1}. {s.strip()}\" for i, s in enumerate(inst_list))\n",
        "            except:\n",
        "                inst_text = instructions\n",
        "        else:\n",
        "            inst_text = str(instructions)\n",
        "\n",
        "        # Format for training\n",
        "        final_text = f\"Recipe: {title}\\nIngredients: {ingr_text}\\nInstructions: {inst_text}\\n\"\n",
        "        return {\"text\": final_text}\n",
        "\n",
        "    # Apply processing\n",
        "    processed_dataset = dataset.map(process_example, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "    return processed_dataset\n",
        "\n",
        "# Load the data\n",
        "csv_path = \"/content/recipes_dataset/3A2M_EXTENDED.csv\"\n",
        "dataset = prepare_data(csv_path)\n",
        "\n",
        "print(\"Processed data sample:\")\n",
        "print(dataset[\"train\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "GqPaVC4uO7Hq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382,
          "referenced_widgets": [
            "9735291a610246ffac01e447f5586265",
            "9ae8b4c2e07f47d883faf0e00ec805d2",
            "d7e6f0c76aa84fccbfb5fb54dbc7321d",
            "7f8853dfef6e4c589480b9701510aa82",
            "fc54f5bcdcad4a0185e3ba8db2c51232",
            "c0815722f51b43d5abac98eed3ec1b41",
            "afd00a79a57c44359abed5f4bfc9ac56",
            "e4837c4f95164540964174407ec40f2a",
            "7c35a1122e9f49118d0227b9d0affb1f",
            "7f87c8f96fe0464ba6eaebc446ae054c",
            "a32d14a26b2e4e37b4dae795a8c867e3"
          ]
        },
        "outputId": "999bc7af-4835-46ae-f23d-b62b7b39ce0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "ðŸ“Š Loading dataset...\n",
            "Dataset sample:\n",
            "{'title': '\\t Arugula Pomegranate Salad', 'NER': '[\"baby spinach\", \"baby arugula\", \"pomegranate arils\", \"persimmon\", \"alfalfa sprouts\"]', 'Extended_NER': \"['alfalfa sprouts', 'baby spinach', 'baby arugula', 'pomegranate arils', 'persimmon']\", 'genre': 'vegetables', 'label': 4, 'directions': '[\"Toss together spinach and arugula, then place in your serving bowl.\", \"Remove the stem and leaves of the persimmon, then slice into thin wedges.\", \"Arrange the persimmon on top of the spinach and arugula.\", \"Garnish with pomegranate arils and alfalfa sprouts.\"]'}\n",
            "\n",
            "Columns: ['title', 'NER', 'Extended_NER', 'genre', 'label', 'directions']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2231143 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9735291a610246ffac01e447f5586265"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data sample:\n",
            "Recipe: Arugula Pomegranate Salad\n",
            "Ingredients: - baby spinach\n",
            "- baby arugula\n",
            "- pomegranate arils\n",
            "- persimmon\n",
            "- alfalfa sprouts\n",
            "Instructions: 1. Toss together spinach and arugula, then place in your serving bowl.\n",
            "2. Remove the stem and leaves of the persimmon, then slice into thin wedges.\n",
            "3. Arrange the persimmon on top of the spinach and arugula.\n",
            "4. Garnish with pomegranate arils and alfalfa sprouts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# TASK 2: DECODER MODEL (GPT-2) - RECIPE GENERATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TASK 2: GPT-2 RECIPE GENERATION\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "WuuF8S44O9cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ea102b-1f15-4eff-aa75-1dde5ee63689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TASK 2: GPT-2 RECIPE GENERATION\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 1. Tokenization and Dataset Formatting (Optimized)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nðŸ”¤ Step 1: Tokenization and dataset formatting...\")\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ðŸ”„ Using device: {device}\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the dataset for training\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenize the dataset (this happens on CPU - which is normal and optimal)\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1000,  # Process in larger batches for efficiency\n",
        "    remove_columns=[\"text\"],\n",
        "    desc=\"Tokenizing dataset\"\n",
        ")\n",
        "\n",
        "# Set format for PyTorch and prepare for GPU\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "print(f\"âœ… Tokenization complete! Samples: {len(tokenized_dataset['train'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "0d648461710e45fdb5991c6df3c73a49",
            "a7086f9dcb99434b9b36cbab63b8f100",
            "bf2c3edc6c034d70a33ac1dba31f3eea",
            "28ea46ad92814a55b952778aaf8f587d",
            "08a3d686b38d4ee683972eec8b05e764",
            "ec4968e8a0284182849caf786b77edf6",
            "88b63cea8e6a43c8be0590de0e1f5abc",
            "5d8cc60f37d5492babb51ae93f759f38",
            "62bfc2583a7f4c16a2e6a5d27fccc714",
            "22821b1277cd4f81a7751c789b530eb3",
            "d7f22dd90eae433db79bcb8f79d5e449"
          ]
        },
        "id": "c7vGKn1uezRf",
        "outputId": "9fe82775-4362-42e7-cad2-3b7f62f6cb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¤ Step 1: Tokenization and dataset formatting...\n",
            "ðŸ”„ Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing dataset:   0%|          | 0/2231143 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d648461710e45fdb5991c6df3c73a49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Tokenization complete! Samples: 2231143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2. GPU-OPTIMIZED TRAINING (Training on 5000 samples)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nðŸŽ¯ Step 2: Setting up GPU-optimized training on 5000 samples...\")\n",
        "\n",
        "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Use exactly 5000 samples from the dataset\n",
        "dataset_chunk_size = 500\n",
        "print(f\"ðŸ“Š Using {dataset_chunk_size} samples\")\n",
        "\n",
        "# Create chunk of dataset - make sure we don't exceed available data\n",
        "available_samples = len(tokenized_dataset[\"train\"])\n",
        "if dataset_chunk_size > available_samples:\n",
        "    dataset_chunk_size = available_samples\n",
        "    print(f\"âš ï¸  Only {available_samples} samples available, using all\")\n",
        "\n",
        "dataset_chunk = tokenized_dataset[\"train\"].select(range(dataset_chunk_size))\n",
        "\n",
        "# Split chunk into train/validation\n",
        "train_size = int(0.9 * len(dataset_chunk))  # 90% train, 10% validation\n",
        "train_dataset = dataset_chunk.select(range(train_size))\n",
        "eval_dataset = dataset_chunk.select(range(train_size, len(dataset_chunk)))\n",
        "\n",
        "print(f\"ðŸ“Š Dataset split - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
        "\n",
        "# ðŸŽ¯ THIS IS WHERE GPU GETS USED - Load model and move to GPU\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.to(device)  # Move model to GPU\n",
        "print(f\"âœ… Model loaded and moved to {device}\")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal language modeling\n",
        ")\n",
        "\n",
        "# Training arguments optimized for 5000 samples\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./recipe-gpt2-5k\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,  # Good batch size for 5000 samples\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=20,    # More frequent logging for smaller dataset\n",
        "    eval_steps=50,       # More frequent evaluation\n",
        "    save_steps=100,\n",
        "\n",
        "    # Evaluation strategy\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    # ðŸŽ¯ GPU OPTIMIZATIONS\n",
        "    fp16=True,  # Mixed precision training\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    # Logging\n",
        "    report_to=None,\n",
        "    logging_dir=\"./logs-5k\",\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Verify GPU is being used\n",
        "print(f\"ðŸŽ¯ Model device: {next(model.parameters()).device}\")\n",
        "print(f\"ðŸŽ¯ Training batch size per GPU: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"ðŸŽ¯ Total training steps: {len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")\n",
        "\n",
        "# Start training - THIS WILL USE GPU\n",
        "print(\"ðŸš€ Starting GPU training on 5000 samples...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"âœ… Training completed!\")\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model()\n",
        "print(\"ðŸ’¾ Model saved to './recipe-gpt2-5k'\")"
      ],
      "metadata": {
        "id": "vhVkN8HyO_7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff72d23d-9322-491f-e4c5-2debdbce2344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Step 2: Setting up GPU-optimized training on 5000 samples...\n",
            "ðŸ“Š Using 5000 samples\n",
            "ðŸ“Š Dataset split - Train: 4500, Eval: 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1503545265.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model loaded and moved to cuda\n",
            "ðŸŽ¯ Model device: cuda:0\n",
            "ðŸŽ¯ Training batch size per GPU: 8\n",
            "ðŸŽ¯ Total training steps: 1686\n",
            "ðŸš€ Starting GPU training on 5000 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1689/1689 44:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.543700</td>\n",
              "      <td>2.287907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.350500</td>\n",
              "      <td>2.223342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.314000</td>\n",
              "      <td>2.204077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.275100</td>\n",
              "      <td>2.181569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.267200</td>\n",
              "      <td>2.164154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.287200</td>\n",
              "      <td>2.155835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.207500</td>\n",
              "      <td>2.150912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.250200</td>\n",
              "      <td>2.142655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.246000</td>\n",
              "      <td>2.127900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.107400</td>\n",
              "      <td>2.124775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.162500</td>\n",
              "      <td>2.110606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.087800</td>\n",
              "      <td>2.121486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.131700</td>\n",
              "      <td>2.107921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.102900</td>\n",
              "      <td>2.100105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>2.081300</td>\n",
              "      <td>2.098717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.060600</td>\n",
              "      <td>2.094194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.091600</td>\n",
              "      <td>2.088945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.068000</td>\n",
              "      <td>2.084570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.074700</td>\n",
              "      <td>2.082624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.058000</td>\n",
              "      <td>2.078663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>2.059900</td>\n",
              "      <td>2.076620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.015300</td>\n",
              "      <td>2.075084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>2.005500</td>\n",
              "      <td>2.071273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.028000</td>\n",
              "      <td>2.068683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>2.034000</td>\n",
              "      <td>2.071753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.993300</td>\n",
              "      <td>2.069798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.954800</td>\n",
              "      <td>2.065687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>2.032200</td>\n",
              "      <td>2.066586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>2.071400</td>\n",
              "      <td>2.065806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.987300</td>\n",
              "      <td>2.062928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>2.019000</td>\n",
              "      <td>2.061897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.985600</td>\n",
              "      <td>2.060338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>1.961000</td>\n",
              "      <td>2.061763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training completed!\n",
            "ðŸ’¾ Model saved to './recipe-gpt2-5k'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DELIVERABLE 3: Example Generations and Quality Evaluation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class RecipeGenerator:\n",
        "    def __init__(self, model_path=\"./recipe-gpt2-5k\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        # Set the pad token ID for generation\n",
        "        self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "    def generate_recipe(self, prompt, max_length=300, temperature=0.9):\n",
        "        input_text = f\"Recipe: {prompt}\\nIngredients:\"\n",
        "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.95,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                num_return_sequences=1,\n",
        "                repetition_penalty=1.1\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Initialize your trained generator\n",
        "generator = RecipeGenerator()\n",
        "\n",
        "print(\"ðŸ§ª Generating example recipes from YOUR dataset...\")\n",
        "\n",
        "# Get actual samples from YOUR dataset for testing from the ORIGINAL dataframe\n",
        "test_samples_df = df.head(5) # Use the first 5 samples from the original dataframe\n",
        "\n",
        "example_generations = []\n",
        "for i, row in test_samples_df.iterrows():\n",
        "    # Extract data from the original dataframe row\n",
        "    title = str(row['title']).strip()\n",
        "    original_ingredients = str(row['NER'])\n",
        "    original_instructions = str(row['directions'])\n",
        "\n",
        "    # Generate recipe using your trained model\n",
        "    generated_recipe = generator.generate_recipe(title)\n",
        "\n",
        "    example_generations.append({\n",
        "        \"original_title\": title,\n",
        "        \"original_ingredients\": original_ingredients,\n",
        "        \"original_instructions\": original_instructions,\n",
        "        \"generated_recipe\": generated_recipe\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXAMPLE {i+1}: {title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"ðŸ“ GENERATED RECIPE:\")\n",
        "    print(generated_recipe)\n",
        "    print(f\"\\nðŸ“š ORIGINAL INGREDIENTS: {original_ingredients[:100]}...\")\n",
        "    print(f\"ðŸ“š ORIGINAL INSTRUCTIONS: {original_instructions[:100]}...\")\n",
        "\n",
        "# Quality Evaluation\n",
        "print(\"\\nðŸ“Š Running quality evaluation on YOUR data...\")\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import json\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def evaluate_on_actual_data(generator, test_samples_count=5):\n",
        "    \"\"\"Evaluate using actual samples from your dataset\"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Get actual samples from the ORIGINAL dataframe for evaluation\n",
        "    actual_samples_df = df.head(test_samples_count)\n",
        "\n",
        "\n",
        "    rouge_scores = []\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i, row in actual_samples_df.iterrows():\n",
        "        title = str(row['title']).strip()\n",
        "        ingredients = str(row['NER'])\n",
        "        instructions = str(row['directions'])\n",
        "\n",
        "        # Create reference text from original data\n",
        "        ingr_text = \"\"\n",
        "        if isinstance(ingredients, str) and ingredients.startswith(\"[\") and ingredients.endswith(\"]\"):\n",
        "            try:\n",
        "                ing_list = json.loads(ingredients)\n",
        "                ingr_text = \"\\n\".join(f\"- {i.strip()}\" for i in ing_list if i.strip())\n",
        "            except:\n",
        "                ingr_text = ingredients\n",
        "        else:\n",
        "            ingr_text = ingredients\n",
        "\n",
        "        inst_text = \"\"\n",
        "        if isinstance(instructions, str) and instructions.startswith(\"[\") and instructions.endswith(\"]\"):\n",
        "            try:\n",
        "                inst_list = json.loads(instructions)\n",
        "                inst_text = \"\\n\".join(f\"{i+1}. {s.strip()}\" for i, s in enumerate(inst_list))\n",
        "            except:\n",
        "                inst_text = instructions\n",
        "        else:\n",
        "            inst_text = instructions\n",
        "\n",
        "\n",
        "        reference_text = f\"Recipe: {title}\\nIngredients:\\n{ingr_text}\\nInstructions:\\n{inst_text}\\n\"\n",
        "\n",
        "        # Generate recipe\n",
        "        generated_text = generator.generate_recipe(title)\n",
        "\n",
        "        # Calculate scores\n",
        "        rouge_score = scorer.score(reference_text, generated_text)\n",
        "        rouge_scores.append(rouge_score)\n",
        "\n",
        "        # BLEU score - Ensure both reference and candidate are strings before splitting\n",
        "        reference_tokens = [str(reference_text).split()]\n",
        "        candidate_tokens = str(generated_text).split()\n",
        "        try:\n",
        "            bleu = sentence_bleu(reference_tokens, candidate_tokens, weights=(0.5, 0.5, 0, 0))\n",
        "            bleu_scores.append(bleu)\n",
        "        except:\n",
        "            bleu_scores.append(0.0)\n",
        "\n",
        "\n",
        "    return {\n",
        "        'rouge1': sum(s['rouge1'].fmeasure for s in rouge_scores) / len(rouge_scores),\n",
        "        'rouge2': sum(s['rouge2'].fmeasure for s in rouge_scores) / len(rouge_scores),\n",
        "        'rougeL': sum(s['rougeL'].fmeasure for s in rouge_scores) / len(rouge_scores),\n",
        "        'bleu': sum(bleu_scores) / len(bleu_scores),\n",
        "        'num_samples': test_samples_count\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_on_actual_data(generator, test_samples_count=5)\n",
        "\n",
        "print(\"\\nðŸ“ˆ QUALITY EVALUATION RESULTS:\")\n",
        "print(f\"ROUGE-1: {evaluation_results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {evaluation_results['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {evaluation_results['rougeL']:.4f}\")\n",
        "print(f\"BLEU: {evaluation_results['bleu']:.4f}\")\n",
        "print(f\"Evaluated on: {evaluation_results['num_samples']} samples from YOUR dataset\")\n",
        "\n",
        "print(\"âœ… DELIVERABLE 3 COMPLETE: Example generations and evaluation done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXydUc49CRmI",
        "outputId": "36141481-ed3b-4a0c-cc6a-7fbbcfb75320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DELIVERABLE 3: Example Generations and Quality Evaluation\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Generating example recipes from YOUR dataset...\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 1: Arugula Pomegranate Salad\n",
            "============================================================\n",
            "ðŸ“ GENERATED RECIPE:\n",
            "Recipe: Arugula Pomegranate Salad\n",
            "Ingredients: - pomegranate rind\n",
            "- cilantro\n",
            "Instructions; strain.\n",
            "Set aside for 5 minutes.\n",
            "In a bowl combine ingredients, stir.\n",
            "Brush with salt and pepper.\n",
            "Put in refrigerator until firm to serve. Makes enough to cover all but 1 cup of salad.\n",
            "Ingredients: - pomegratias\n",
            "2 tbs olive oil\n",
            "8 Tbsp lime juice\n",
            "4 garlic cloves & 1/2 tsp ground black pepper\n",
            "1 1/2 tsp ground mustard powder\n",
            "Pomegranates (optional) 4 oz Fresh squeezed lemon juice\n",
            "Juice from lemon zest\n",
            "Instructions Cut the pomegranacy into bite size pieces.\n",
            "Heat oil in skillet over medium heat.\n",
            "Add pomelo flakes. Bring to a boil. Reduce heat to low and cook until mixture thickens, 20 seconds. Remove pan from heat.\n",
            "Add remaining ingredients if desired consistency. Stir and stir constantly until mixture has thickened and set aside 2 minutes.\n",
            "3 min longer.\n",
            "Remove pan from heat. Add reserved poms or add more water if desired. Sprinkle with additional salt and crushed pecans if desired flavor(optional). Serve hot or cold.\n",
            "6.5 oz. Toasted bread crumbs\n",
            "Place sour cream on top layer of sliced breadcrumb mixture.\n",
            "Instructions: Combine sour creme jam & sour cream.\n",
            "Pour sour cream\n",
            "\n",
            "ðŸ“š ORIGINAL INGREDIENTS: [\"baby spinach\", \"baby arugula\", \"pomegranate arils\", \"persimmon\", \"alfalfa sprouts\"]...\n",
            "ðŸ“š ORIGINAL INSTRUCTIONS: [\"Toss together spinach and arugula, then place in your serving bowl.\", \"Remove the stem and leaves ...\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 2: Black Bean And Turkey Chili\n",
            "============================================================\n",
            "ðŸ“ GENERATED RECIPE:\n",
            "Recipe: Black Bean And Turkey Chili\n",
            "Ingredients: - ground beef\n",
            "- tomato paste\n",
            "Instructions: 1. Cook meat in a 2-quart saucepan over medium heat until juices run clear and water runs clear, about 10 minutes. Drain.\n",
            "2..Cut the lamb into strips lengthwise (about 8\" apart) and place it in the crockpot. Brown the onions and then drain. Add tomato paste to the crook and cook on high for 20 to 25 minutes or possibly longer, stirring occasionally. Toss the beef with water and stir gently. Heat to boiling. Remove from heat and reduce heat to low when you see cooked beef starting its \"stir\" turn green. Serve immediately topped with cheddar cheese, if desired!\n",
            "3.**Add remaining ingredients as needed for chili.\n",
            "4. *Add any leftover stewing liquid while simmering the beef.\n",
            "5......add more spices...if desired!**\n",
            "6.(Use pepperoni mixture instead of sour cream.) I used shredded Mozzarella cheese but feel free :-)\n",
            "7....\n",
            "8.....or use chopped chives(if using). Or corn tortillas with extra cheese added. For a more fun way to eat, substitute salsa verde for red onion slices and add more diced green peppers etc.(makes 12 servings.\n",
            "9 **Note : This recipe was developed by myself without any prior experience with black beans, except for a very light vegetarian diet that doesn't include the black bean/\n",
            "\n",
            "ðŸ“š ORIGINAL INGREDIENTS: [\"olive oil\", \"yellow onion\", \"garlic\", \"ground turkey\", \"black beans\", \"tomatoes\", \"tomato paste\", ...\n",
            "ðŸ“š ORIGINAL INSTRUCTIONS: [\"Dice the onion and mince the garlic. Add the onion and garlic to a large stock pot with one tables...\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 3: Finger Lickin' Tofu Nuggets\n",
            "============================================================\n",
            "ðŸ“ GENERATED RECIPE:\n",
            "Recipe: Finger Lickin' Tofu Nuggets\n",
            "Ingredients: - ginger ale\n",
            "- saltwater\n",
            "Instructions: 1. Place yeast in food processor.\n",
            "2. Add remaining ingredients and process for a few minutes.\n",
            "3. Pour into small bowl. Stir well. Chill till firm.\n",
            "4. Makes 4 servings.\n",
            "5. Store in airtight container.\n",
            "6. Delicious!\n",
            "7. Can be served fresh or frozen.\n",
            "8 Report back with comments, suggestions, and recipes (at least 10 min.).\n",
            "9.\"Stuffed\" (not greasy).\n",
            "10.(If you don't want to bake as it will look messy.) Recipe developed by @BunnyMarlon .\n",
            "11(Fry up leftover potatoes on the stove top first before cooking them...it makes more than enough grease for an entire day's cooking.)\n",
            "\n",
            "ðŸ“š ORIGINAL INGREDIENTS: [\"extra firm\", \"almond flour\", \"nutritional yeast\", \"garlic\", \"unsweetened almond milk\", \"coconut oi...\n",
            "ðŸ“š ORIGINAL INSTRUCTIONS: [\"Wrap the tofu in a clean tea towel and press to remove excess water.\", \"Combine all other ingredie...\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 4: Jerk Beef Stew With Carrots And Tomatoes\n",
            "============================================================\n",
            "ðŸ“ GENERATED RECIPE:\n",
            "Recipe: Jerk Beef Stew With Carrots And Tomatoes\n",
            "Ingredients: - green bell pepper\n",
            "- garlic salt plus\n",
            "Instructions : 1. Bring to a boil over medium heat and cook until tender. Drain.\n",
            "2.. In large saucepan, add parsley flakes, celery, salt and pepper. Simmer 10 minutes.\n",
            "3 .. or at least 5 minutes.\n",
            "4 ....(I always put in boiling water for 8 min).\n",
            "5. Remove from heat and let cool down. Drain.\n",
            "6 . In small bowl mix together remaining ingredients.\n",
            "7 Mix well. Serve warm with rice. Enjoy!\n",
            "8 .\n",
            "9 . Enjoy! \n",
            "\n",
            "ðŸ“š ORIGINAL INGREDIENTS: [\"olive oil\", \"boneless beef chuck\", \"onion\", \"Progresso\", \"jerk sauce\", \"allspice\", \"cinnamon\", \"th...\n",
            "ðŸ“š ORIGINAL INSTRUCTIONS: [\"Preheat oven to 350 degrees F.\", \"Heat the olive oil in a Dutch oven over medium-high heat. Add th...\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 5: Pomegranate Couscous Salad\n",
            "============================================================\n",
            "ðŸ“ GENERATED RECIPE:\n",
            "Recipe: Pomegranate Couscous Salad\n",
            "Ingredients: - cucumbers\n",
            "- olive oil plus water\n",
            "Instructions. Bring all vegetables to boil with a little olive oil and water (a small amount is enough to cover the bottom of a large bowl). Pour in salad dressing. Cover and refrigeration indefinitely. Serve immediately or arrange over rice.\n",
            "Bread: Tapioca, Tabasco cheese, oregano\n",
            "(Cajun/Mexican corn) avocado slices\n",
            "Pomegranates & parsley\n",
            "Instructions for serving.\n",
            "Ingredients needed : 1 cup heavy cream\n",
            "2 tablespoons butter cooking spray\n",
            "3 tbsp salt per package directions.\n",
            "4 cups whole wheat flour\n",
            "5 eggs / oil\n",
            "6 tablespoons sugar\n",
            "7 teaspoons cinnamon\n",
            "8 tablespoon baking powder base\n",
            "9 handfuls green peppers\n",
            "10 garlic clove\n",
            "11 egg whites\n",
            "12 tsp cayenne pepper\n",
            "13 handful fresh cilantro\n",
            "14 garlic\n",
            "15 eggs yolks\n",
            "16 handful salt\n",
            "17 egg whites\n",
            "18 tsp Cajun/Mexicana seasoning\n",
            "19 handful white wine vinegar\n",
            "20 cucumber/pepper\n",
            "21 egg whites\n",
            "22 tsp ground coriander flakes\n",
            "23 cloves salt n paprika\n",
            "24 small handful black olives\n",
            "25 garlic vinaigrette\n",
            "26 basil\n",
            "27 pomegranata slices bread crust\n",
            "28\n",
            "29 ingredients - water\n",
            "30 chicken\n",
            "31 garlic paste\n",
            "32 olive oil\n",
            "33 medium/large mushroom\n",
            "34\n",
            "\n",
            "ðŸ“š ORIGINAL INGREDIENTS: [\"pomegranate arils\", \"whole wheat couscous\", \"extra virgin olive oil\", \"cilantro\", \"cranberries\", \"...\n",
            "ðŸ“š ORIGINAL INSTRUCTIONS: [\"Place couscous in a bowl with 11/2 cups of hot water. Cover bowl with plastic wrap and let set for...\n",
            "\n",
            "ðŸ“Š Running quality evaluation on YOUR data...\n",
            "\n",
            "ðŸ“ˆ QUALITY EVALUATION RESULTS:\n",
            "ROUGE-1: 0.2908\n",
            "ROUGE-2: 0.0643\n",
            "ROUGE-L: 0.1538\n",
            "BLEU: 0.0991\n",
            "Evaluated on: 5 samples from YOUR dataset\n",
            "âœ… DELIVERABLE 3 COMPLETE: Example generations and evaluation done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DELIVERABLE 4: Gradio App for Interactive Recipe Generation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!pip install gradio -q\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "def create_recipe_generator_app(generator):\n",
        "    \"\"\"Create interactive Gradio app\"\"\"\n",
        "\n",
        "    def generate_recipe_interactive(prompt):\n",
        "        try:\n",
        "            recipe = generator.generate_recipe(prompt)\n",
        "            return recipe\n",
        "        except Exception as e:\n",
        "            return f\"Error generating recipe: {str(e)}\"\n",
        "\n",
        "    # Get some actual dish names from your dataset for examples\n",
        "    example_dishes = []\n",
        "    # Use the original dataframe `df` to get the titles\n",
        "    for i in range(min(5, len(df))):\n",
        "        title = str(df['title'][i]).strip()\n",
        "        if title and title != \"Unknown Recipe\":\n",
        "            example_dishes.append([title])\n",
        "\n",
        "    demo = gr.Interface(\n",
        "        fn=generate_recipe_interactive,\n",
        "        inputs=[\n",
        "            gr.Textbox(\n",
        "                label=\"ðŸ³ What would you like to cook?\",\n",
        "                placeholder=\"Enter a dish name (e.g., Chocolate Cake) or ingredients...\",\n",
        "                value=\"Chocolate Chip Cookies\"\n",
        "            )\n",
        "        ],\n",
        "        outputs=gr.Textbox(\n",
        "            label=\"ðŸ‘©â€ðŸ³ Generated Recipe\",\n",
        "            lines=10,\n",
        "            placeholder=\"Your AI-generated recipe will appear here...\"\n",
        "        ),\n",
        "        title=\"AI Recipe Generator - Fine-tuned on 3A2M Dataset\",\n",
        "        description=\"\"\"Generate cooking recipes using GPT-2 fine-tuned on the 3A2M Extended Recipe dataset!\n",
        "                   This model was trained on real recipe data and can create new recipes based on dish names or ingredients.\"\"\",\n",
        "        examples=example_dishes if example_dishes else [\n",
        "            [\"Chocolate Chip Cookies\"],\n",
        "            [\"Vegetable Stir Fry\"],\n",
        "            [\"Chicken Alfredo Pasta\"]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Create the app\n",
        "# Ensure the generator is initialized with the correct model path\n",
        "generator = RecipeGenerator(model_path=\"./recipe-gpt2-5k\") # Using the model saved in the previous step\n",
        "app = create_recipe_generator_app(generator)\n",
        "print(\"âœ… Gradio app created!\")\n",
        "\n",
        "# Save app script\n",
        "app_script = '''\n",
        "import gradio as gr\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "class RecipeGenerator:\n",
        "    def __init__(self, model_path=\"./recipe-gpt2-5k\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate_recipe(self, prompt):\n",
        "        input_text = f\"Recipe: {prompt}\\\\nIngredients:\"\n",
        "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=300,\n",
        "                temperature=0.9,\n",
        "                do_sample=True,\n",
        "                top_p=0.95,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def create_app():\n",
        "    # Ensure the generator is initialized with the correct model path\n",
        "    generator = RecipeGenerator(model_path=\"./recipe-gpt2-5k\")\n",
        "\n",
        "    def generate_recipe_interface(prompt):\n",
        "        return generator.generate_recipe(prompt)\n",
        "\n",
        "    return gr.Interface(\n",
        "        fn=generate_recipe_interface,\n",
        "        inputs=[\n",
        "            gr.Textbox(label=\"Dish Name or Ingredients\")\n",
        "        ],\n",
        "        outputs=gr.Textbox(label=\"Generated Recipe\", lines=10),\n",
        "        title=\"AI Recipe Generator\",\n",
        "        examples=[[\"Chocolate Cake\"], [\"Vegetable Stir Fry\"], [\"Chicken Curry\"]]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_app()\n",
        "    app.launch(share=True)\n",
        "'''\n",
        "\n",
        "with open(\"recipe_gradio_app.py\", \"w\") as f:\n",
        "    f.write(app_script)\n",
        "\n",
        "print(\"âœ… DELIVERABLE 4 COMPLETE: Gradio app ready!\")\n",
        "\n",
        "# Launch the app\n",
        "print(\"\\nðŸš€ Launching Gradio app...\")\n",
        "app.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "ZMzFCOXUDgHP",
        "outputId": "1f1387bf-2bba-4a9c-b635-845112f477ce"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DELIVERABLE 4: Gradio App for Interactive Recipe Generation\n",
            "============================================================\n",
            "âœ… Gradio app created!\n",
            "âœ… DELIVERABLE 4 COMPLETE: Gradio app ready!\n",
            "\n",
            "ðŸš€ Launching Gradio app...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a2caf25f443e5097fa.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a2caf25f443e5097fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ‰ ALL DELIVERABLES COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nðŸ“ FILES CREATED:\")\n",
        "print(\"âœ… 1. recipe_tokenization_script.py - Tokenization and dataset formatting\")\n",
        "print(\"âœ… 2. gpt2_training_script.py - Training loop for GPT-2\")\n",
        "print(\"âœ… 3. recipe_gradio_app.py - Interactive Gradio app\")\n",
        "print(\"âœ… 4. Example generations printed above\")\n",
        "print(\"âœ… 5. Quality evaluation completed with ROUGE/BLEU scores\")\n",
        "\n",
        "print(f\"\\nðŸ“Š YOUR MODEL PERFORMANCE:\")\n",
        "print(f\"   - ROUGE-1: {evaluation_results['rouge1']:.4f}\")\n",
        "print(f\"   - ROUGE-2: {evaluation_results['rouge2']:.4f}\")\n",
        "print(f\"   - ROUGE-L: {evaluation_results['rougeL']:.4f}\")\n",
        "print(f\"   - BLEU: {evaluation_results['bleu']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZLrxoJKEO25",
        "outputId": "a7d6573d-2010-4b4b-f7ec-7498fa103586"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸŽ‰ ALL DELIVERABLES COMPLETED!\n",
            "============================================================\n",
            "\n",
            "ðŸ“ FILES CREATED:\n",
            "âœ… 1. recipe_tokenization_script.py - Tokenization and dataset formatting\n",
            "âœ… 2. gpt2_training_script.py - Training loop for GPT-2\n",
            "âœ… 3. recipe_gradio_app.py - Interactive Gradio app\n",
            "âœ… 4. Example generations printed above\n",
            "âœ… 5. Quality evaluation completed with ROUGE/BLEU scores\n",
            "\n",
            "ðŸ“Š YOUR MODEL PERFORMANCE:\n",
            "   - ROUGE-1: 0.2908\n",
            "   - ROUGE-2: 0.0643\n",
            "   - ROUGE-L: 0.1538\n",
            "   - BLEU: 0.0991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a zip file of your trained model\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_and_download_model():\n",
        "    \"\"\"Zip the model folder and provide download link\"\"\"\n",
        "    model_path = \"./recipe-gpt2-5k\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        # Create zip file\n",
        "        shutil.make_archive(\"recipe-gpt2-5k\", 'zip', model_path)\n",
        "\n",
        "        print(\"âœ… Model zipped successfully!\")\n",
        "        print(\"ðŸ“ File: recipe-gpt2-5k.zip\")\n",
        "\n",
        "        # For Google Colab\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(\"recipe-gpt2-5k.zip\")\n",
        "            print(\"ðŸ“¥ Download started automatically in Colab\")\n",
        "        except:\n",
        "            print(\"ðŸ“ If you're in Colab, the download should start automatically\")\n",
        "            print(\"ðŸ“ If you're in local Jupyter, right-click and download the file\")\n",
        "    else:\n",
        "        print(\"âŒ Model folder not found. Let me check what's available:\")\n",
        "        print(os.listdir(\".\"))\n",
        "\n",
        "# Run the download\n",
        "zip_and_download_model()"
      ],
      "metadata": {
        "id": "-s_EKwEXRxuk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}