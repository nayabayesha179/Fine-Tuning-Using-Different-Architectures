# -*- coding: utf-8 -*-
"""NLP_24f7807_Task1_P2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eR_vj77YeeiDhYKbLLKcY3Plqt_NeSWE
"""

!pip install transformers datasets evaluate scikit-learn pandas matplotlib seaborn accelerate -q

import torch
torch.cuda.is_available(), torch.cuda.get_device_name(0)

from google.colab import files
uploaded = files.upload()  # choose your CSV file

import pandas as pd

df_data = pd.read_csv("emotions-dataset.csv")   # adjust name if .tsv or .txt
df_labels = pd.read_csv("emotion_labels.csv")

print("Dataset columns:", df_data.columns)
print("Label file columns:", df_labels.columns)
df_data.head(), df_labels.head()

print("df_data columns:", df_data.columns.tolist())
print("df_labels columns:", df_labels.columns.tolist())

# Also show the first few rows
print("\nSample from df_data:")
print(df_data.head())

print("\nSample from df_labels:")
print(df_labels.head())

import pandas as pd

# Merge using sentiment (data) and label (mapping)
df = pd.merge(df_data, df_labels, left_on="sentiment", right_on="label", how="left")

# Keep only useful columns and rename for consistency
# Explicitly select the 'emotion' column from df_labels and rename it to 'label'
df = df[['content', 'emotion']].rename(columns={"content": "text", "emotion": "label"})

df.head()

type(df['label'])

print(type(df['label']))
print(df['label'].head())
print(df['label'].unique())

from sklearn.model_selection import train_test_split
import re

def clean_text(s):
    s = str(s).strip()
    s = re.sub(r'\s+', ' ', s)
    return s

df['text'] = df['text'].apply(clean_text)
df = df.dropna()

# map labels to numeric IDs
label2id = {label: idx for idx, label in enumerate(sorted(df['label'].unique()))}
id2label = {v: k for k, v in label2id.items()}
df['label_id'] = df['label'].map(label2id)

# split
train_df, test_df = train_test_split(df, test_size=0.1, stratify=df['label_id'], random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['label_id'], random_state=42)

len(train_df), len(val_df), len(test_df)

from datasets import Dataset, DatasetDict

train_ds = Dataset.from_pandas(train_df[['text', 'label_id']])
val_ds   = Dataset.from_pandas(val_df[['text', 'label_id']])
test_ds  = Dataset.from_pandas(test_df[['text', 'label_id']])

dataset = DatasetDict({
    "train": train_ds,
    "validation": val_ds,
    "test": test_ds
})
dataset

from transformers import AutoTokenizer

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(batch):
    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)

tokenized = dataset.map(tokenize, batched=True)
tokenized = tokenized.rename_column("label_id", "labels")
tokenized.set_format("torch", columns=["input_ids", "attention_mask", "labels"])



from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate
import numpy as np

num_labels = len(label2id)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    }

training_args = TrainingArguments(
    output_dir="./checkpoints_bert",
    eval_strategy="epoch",  # Changed from evaluation_strategy
    save_strategy="epoch",
    load_best_model_at_end=True,
    learning_rate=3e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    metric_for_best_model="f1",
    fp16=torch.cuda.is_available()
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "offline"
os.environ["WANDB_SILENT"] = "true"

trainer.train()

metrics = trainer.evaluate(tokenized["test"])
metrics

import torch

def predict(text):
    tokens = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    tokens = {k: v.to(model.device) for k,v in tokens.items()}
    with torch.no_grad():
        outputs = model(**tokens)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    pred_id = torch.argmax(probs, dim=-1).item()
    return {
        "text": text,
        "pred_label": id2label[pred_id],
        "probs": {id2label[i]: float(probs[0][i]) for i in range(num_labels)}
    }

# Try a few examples
examples = ["I'm thrilled about this news!",
    "Feeling fantastic today!",
    "What a wonderful experience!",
    "This made me so happy!",
    "I'm smiling from ear to ear!", "I am so happy today!", "This makes me furious!", "I'm feeling a bit down.", "Hi, how are you?", "i hate you.", "yeyyy! i finally won this match", "Get out of here"]
for ex in examples:
    print(predict(ex))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

preds = trainer.predict(tokenized["test"])
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=-1)

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[id2label[i] for i in range(num_labels)])
disp.plot(cmap='Blues', xticks_rotation=45)
plt.show()

from sklearn.metrics import classification_report
import numpy as np

preds = trainer.predict(tokenized["test"])
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=-1)

from sklearn.metrics import classification_report
print(classification_report(
    y_true,
    y_pred,
    target_names=[id2label[i] for i in range(len(id2label))],
    digits=3
))

# ðŸ§  Save fine-tuned model and tokenizer
save_dir = "bert_emotion_model"

model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print(f"Model and tokenizer saved in folder: {save_dir}")

!zip -r bert_emotion_model.zip bert_emotion_model
from google.colab import files
files.download("bert_emotion_model.zip")